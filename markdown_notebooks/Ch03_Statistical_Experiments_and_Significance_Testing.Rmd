---
title: "Chapter 3: Statistical Experiments and Significance Testing"
output:
  html_notebook:
    toc: true
    toc_float: true
    theme: cerulean
    highlight: tango
---

```{r setup, warning = FALSE, message = FALSE}
# Load libraries
library(tidyverse)
library(kableExtra)
library(ggtext)
library(RColorBrewer)
library(modelr)
library(lmPerm)
library(pwr)

# Set ggplot2 theme
my_theme <- theme(
  panel.background = element_rect(fill = "white", color = "black"), 
  panel.grid = element_blank(),
  plot.title = element_markdown(),
  plot.subtitle = element_markdown(),
  plot.caption = element_markdown()
)

```

## Introduction 

The basic workflow in any statistical analysis is to first formulate a hypothesis, design an experiment to test the hypothesis, collect data, analyze the data, and make conclusions and inferences inferences to the larger population that the sample was drawn from.

## A/B Testing

A/B testing involves the randomized allocation of subjects to treatment groups (those subjects that receive the treatment) and control groups (those subjects that don't receive the treatment) and evaluating a test statistic to measure the effects of the treatment.  For example, physicians may be interested in the effectiveness of a drug in treating a disease relative to a placebo, so subjects may be randomly assigned into treatment and control (i.e., placebo) groups to evaluate the drug's effectiveness. In this particular case of a clinical trial, blind or double-blind study designs may be used.  In a blind study design, the subjects are unaware if they were randomly assigned into the treatment or the control group; in a double-blind study design, both the subjects AND those administering the treatment are unaware which group the subjects have been placed in.  In data science, the A/B tests are typically motivated by business interests: evaluating which web page style gets more views; which site headline gets more clicks; or which product price gets more purchases.  Importantly, the researcher must establish the test statistic to measure the effect of the treatment prior to the experiment.

## Hypothesis Tests

Hypothesis tests are critical to evaluate the differences between true effects due to a treatment or similar, or natural random variation.  The idea behind random assignment in A/B tests is that any differences observed between treatment groups can be due to only two things: a) the random change in assignments of subjects to either the experimental or control group; or b) a true effect due to the treatment.  The hypothesis test relies on two hypotheses: the null hypothesis, which assumes that both groups are equal, and that any variation is due to chance; and the alternative hypothesis, which states that both groups are not equal, and that any variation cannot be reasonably attributed to chance alone.  One way in which the null hypothesis is framed is by creating a null distribution, by conducting a permutation test to shuffle subjects between two groups such that the permuted group sizes are the same as the original group sizes.  This is repeated a defined number of times, creating a distribution where there are no differences between groups.  This distribution can then be compared to the empirical data.  

Hypothesis tests can be one-tailed, or two-tailed, depending on the research question.  One-tailed tests only consider a single alternative, while two-tailed tests consider two alternatives.  For example, a typical one-tailed test in data science may involve comparing the current web page headline to a new, flashier web page headline.  In this situation, we may only be interested in whether or not the new web page design attracts more viewers, and if it doesn't, we'll just stick with the current design.  In a different situation, maybe we're starting from scratch and want to know which web page headline background color attracts more viewers: blue, or orange.  In this case, the orange background could attract more viewers, or it could attract less. 

## Resampling

In hypothesis testing, there are two forms of resampling, or redrawing samples from the observed data: bootstrapping, or resampling with replacement, typically to evaluate the variability in an estimate; and permutation, typically involving two or more groups, where observations may be randomly shuffled between groups to generate a null distribution (i.e., a null hypothesis of no difference between groups).  One of the key differences is in permutation testing, there is more than one group being compared, and we're going to be resampling _without_ replacement, because we're shuffling observations between groups to simulate the null hypothesis of no differences between groups.  Another key theme with the permutation test is that the original sample sizez will be maintained in each permutation.  For example, if you want to compare the group means between groups A and B, and there are 50 observations in group A and 48 observations in group B, there will be 50 and 48 observations in the permuted groups A and B, respectively, in each iteration.  With each iteration, the original estimate of the statistic will be recorded to generate a null distribution to compare to the observed value from the original, non-permuted data.  In essence, if the estimate calculated from the empirical distribution is within the null distribution created from the permutations, we may conclude that the estimate could be obtained simply by chance.  However, if the estimate is extreme and lies in one of the tails of the distribution, we may conclude that the chances that the estimate was obtained by chance are rare.  

To look at an example of the permutation test in action, we'll look at the `web_page_data.csv` file, which contains Google Analytics data on the amount of time users spent on a particular experimental web page.  The idea is to identify which web design users spent the most time on to drive business decisions for the hypothetical company.  

```{r google-analytics-sessions-plot}
# Load the four sessiosn data and convert the minutes to seconds
session_times <- read_csv("../data/web_page_data.csv") %>% 
  mutate(Time = Time * 100)

# Create boxplot of the page sessions 
session_times %>% 
  ggplot(aes(Page, Time)) +
  geom_boxplot() +
  my_theme +
  labs(y = "Time (seconds)", 
       title = "Time spent on different web pages",
       subtitle = "Data are from Google Analytics")
```

From this plot, we can see that Page B has the highest median time.  We can also plot the distribution of time spent on each webpage as a density plot and label the mean time spent as a vertical dashed line to visualize differences in means.

```{r google-analytics-sessions-density}
# Plot the distribution of times spent on each webpage and label the means for each page
session_times %>%
  ggplot(aes(Time)) +
  geom_density(aes(fill = Page), 
               alpha = 0.6) +
  geom_vline(data = filter(session_times, Page == "Page A"), aes(xintercept = mean(Time)), linetype = "dashed") +
  geom_vline(data = filter(session_times, Page == "Page B"), aes(xintercept = mean(Time)), linetype = "dashed") +
  my_theme +
  scale_x_continuous(expand = expansion(mult = c(0, 0))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  facet_grid(Page ~ .) +
  theme(
    legend.position = "none"
  ) +
  labs(x = "Time (seconds)", 
       y = "Density", 
       title = "Distribution of times spent on each webpage from Google Analytics data", 
       subtitle = "The mean time spent on each webpage is represented as a dashed line")

```

From this faceted plot, we can see users spent the least amount of time on Page A and the greatest amount of time on Page B.  Let's create a separate data frame to show a table of the mean time spent on each page. 

```{r session-times-table}
# Create a new data frame with just the mean times for each page
mean_session_times <- session_times %>% 
  group_by(Page) %>% 
  summarise(mean_times = mean(Time), 
            n = n())
  
# Display a table of all the mean times per page
mean_session_times %>% 
  kbl(col.names = c("Page", "Mean time", "n"), 
      caption = "Mean time spent on each webpage (data from Google Analytics)") %>% 
  kable_classic_2() %>% 
  kable_styling(full_width = FALSE, 
                "striped")

```

Looking at Page A and Page B, we can see that the difference in mean times spent on the different webpages is about 36 seconds.  Ultimately, we want to know if this difference can be attributed to differences in the webpages, or just random chance.  To do this, we'll create a permutation function (derived from the the same function in the textbook) that will take three arguments: `x`, the data frame to use; `n_A`, the number of samples in group A (e.g., Page 1); and `n_B`, the number of samples in group B.  We'll then create indexes to use in the permutation by randomly sampling numbers from 1 to the `n` (the total number of samples in the dataset) of the size of group B.  After that, we'll get the difference and assign those indexes to group A.  Then, we'll subset each group by randomly shuffling the indexes and calculate the mean for each group.  

```{r permutation-function}
# Create the permutation function
perm_function <- function(x, n_A, n_B) {
  n <- n_A + n_B
  index_B <- sample(1:n, n_B)
  index_A <- setdiff(1:n, index_B)
  mean_diff <- mean(x[index_B]) - mean(x[index_A])
  return(mean_diff)
}

```

After we've created the function, we'll perform 10,000 iterations to generate a null distribution of the data.

```{r permutation-reps}
# Create an empty data frame
perm_diffs <- rep(0, 1000)

# For loop to run the function 10,000 times
for (i in 1:1000) {
  perm_diffs[i] <- perm_function(session_times$Time, 21, 15)
}

# Plot the distribution and our empirical difference in means as a dashed line 
data.frame(permuted_mean_diff = perm_diffs) %>% 
  ggplot(aes(permuted_mean_diff)) +
  geom_density(fill = "lightskyblue", 
               alpha = 0.6) +
  geom_vline(xintercept = 36, 
            linetype = "dashed") +
  my_theme +
  scale_x_continuous(expand = expansion(mult = c(0, 0))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.01))) +
  labs(x = "Mean time difference (seconds)", 
       y = "Density", 
       title = "Distribution of mean time difference (seconds) from permutation test", 
       subtitle = "Mean difference from empirical data is represented as a dashed line")
```

We can now look at the percentage of the mean differences calculated from the permuted data that are as extreme or more extreme than what we observed from the empirical data.

```{r percentage-extreme}
# Calculate the percentage of as extreme or more extreme mean differences
data.frame(permuted_mean_diffs = perm_diffs) %>% 
  filter(permuted_mean_diffs >= 36) %>% 
  nrow()
```

There are 141 mean differences from the permutation data that are as extreme or more extreme than the difference calculated from the empirical data, or about 14%.  This is the p-value, which indicates that the result is not statistically significant.  The permutation test is relatively straightforward and doesn't rely on assumptions of normality, and is thus widely applicable to a variety of datasets.  

## Statistical Significance and p-Values

When conducting hypothesis tests, we want to know the probability that we would obtain the estimate of a particular statistic, if the null hypothesis were true (i.e., no difference between groups).  This, in essence, is the p-value, and it gives us an indication about the unusualness of a particular result, or the chances of observing that particular result by chance if the null hypothesis were true.  We can reuse the permutation function from the section above and apply it to the new example dataset looking at the number of conversions from a table of ecommerce data.  

```{r e-commerce-permutation}
# Generate a vector of the observed percentage of differences between B and A
obs_pct_diff <- 100 * (200 / 23539 - 182 / 22588) # 0.04391621

# Create a vector to store the permutation indexes
# We'll need the total number of non-conversions (45945) and the total number of successful conversions (200 + 182 = 382)
conversion <- c(rep(0, 45945), rep(1, 382))

# Create a vector to store the permutation results
perm_diffs <- rep(0, 1000) # store 1,000 0's....

# Run the function 1,000 times
for (i in 1:1000) {
  perm_diffs[i] <- 100 * perm_function(conversion, 23739, 22588)
}

# Visualize the null distribution 
data.frame(perm_diffs = perm_diffs) %>% 
  ggplot(aes(perm_diffs)) +
  geom_density(fill = "lightskyblue", 
               alpha = 0.6) +
  geom_vline(xintercept = obs_pct_diff, 
             linetype = "dashed") +
  my_theme + 
  scale_x_continuous(expand = expansion(mult = c(0, 0))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.02))) +
  labs(x = "Permuted conversion rate (%)", 
       y = "Density",
       title = "Null distribution of conversion rate (%) between webpages B and A", 
       subtitle = "The observed difference is denoted as a dashed line")
```

Looking at the plot above, it appears to be in the range that's plausible just by chance alone.  We can formalize this conclusion by calculating the p-value.  We'll take the number of observations from the null distribution that are as extreme or more extreme than the one we've observed and divide by the total number of observations in the null distribution (1,000).

```{r calculate-p-value}
# Calculate the p-value by hand
data.frame(perm_diffs) %>% 
  filter(perm_diffs >= obs_pct_diff) %>% 
  nrow() / 1000
```

The p-value is 0.298, meaning we would expect to see a conversion rate as extreme or more extreme than the one we observed about 30% of the time, if the null hypothesis was true.  Since the data are binary (conversion or no conversion), we can also perform a test for equal proportions under the binomial distribution using the `prop.test()` function.

```{r prop-test}
# Perform a test of equal proportions 
prop.test(x = c(200, 182), 
          n = c(23539, 22406), 
          alternative = "greater")

```

From this output we can see the proportion of successes (i.e., conversions, defined in the `x` argument) for each group, as well as the point estimate and 95% confidence interval for the chi-squared test statistic.  The p-value for this test is similar to that obtained from the permutation test and doesn't change the overall interpretation of the results.  

There are two errors commonly discussed in hypothesis testing: Type I errors, or false-positives, in which the researcher rejects the null hypothesis when the null hypothesis is true; and Type II errors, or false negatives, in which the researcher fails to reject the null hypothesis when the null hypothesis is false.  

## t-Tests

The t-test is commonly used in hypothesis testing to evaluate the difference between means of two groups, by comparing the test statistic to a t-distribution.  We can implement this with the webpage data using the `t.test()` function.

```{r t-test}
# Perform a t-test with the webpage data
t.test(Time ~ Page, data = session_times, 
       alternative = "less")

```

From this output, we can see the mean time spent on the different webpages, and a p-value when we are testing the alternative hypothesis that the mean time spent on Page A is less than the time spent on Page B.  

## Multiple Testing

The problem of multiple testing can be explained through a simple formula: take the probability that one test will correctly test non-significant, which is 1 minus the alpha level (e.g., 1 - 0.05 = 0.95); multiply this probability for all tests (e.g., if there are three tests: 0.95 x 0.95 x 0.95); subtract this cumulative probability from 1 to obtained the probability that at least one test will be significant by chance, thus committing a Type I error (i.e., a false positive).  This is known as alpha inflation, which describes the concept that as you increase the number of significance tests, the probability of making a Type I error increases.  Thus, multiple testing corrections need to be employed in an attempt to control this inflation.  This is also a problem when trying to fit statistical models but can be overcome by having two separate datasets: a training set used to train the model, and a validation set composed of data the model has not seen before to evaluate the predictive ability of the model.  

## False Discovery Rate

Broadly, the false discovery rate (FDR) is described as the rate at which hypothesis tests found to be significant are actually false.  For example, if the FDR level was 0.05, that means that in 100 tests identified to be statistically significant, we would ensure that no more than 5 of those tests would be false positives (i.e., Type I error rate).  

## Degrees of Freedom

The degrees of freedom are just the number of values that are free to vary in a sample dataset and influence the shape of a probability distribution.  For example, using the excellent example from the textbook, if you are calculating the mean with 10 integers, there are 9 degrees of freedom.  The idea is that once you know 9 of the values, you can calculate the 10th, and it's not free to vary.  Many estimators will use a bias correction factor, typically $n-1$ in the denominator, because normally, since we are using a sample of the data, the estimates would be biased.  

## ANOVA

The analysis of variance (ANOVA) takes more than two groups and tests if the differences in means are due to chance.  For this section, we'll use the `four_sessions.csv` data, which contains the time (in seconds) spent on each of four different webpages.  

```{r four-sessions-plot}
# Load the four sessions data
four_sessions <- read_csv("../data/four_sessions.csv")

# Create the boxplot
four_sessions %>% 
  ggplot(aes(Page, Time)) +
  geom_boxplot() +
  my_theme +
  labs(title = "Time (seconds) spent on each webage")
```

The basic strategy to test if the group means are the same is use a permutation approach, combining all the data into a single group, then re-sampling into four groups, finding the mean of those new groups, and repeating this process a defined number of times.  To perform this permutation test, we'll use the `aovp()` function from the `lmPerm` package.

```{r lmPerm-aov}
# Perform the ANOVA permutation tests
aov_permutation <- aovp(Time ~ Page, data = four_sessions)

# Get the summary of this object
summary(aov_permutation)
```

These results indicate that we could expect to see the observed differences in the mean time spent on each webpage by chance alone about 8.6% of the time.  Because our p-value is greater than the standard 0.05, we can conclude that any differences we observed are just due to chance.  

## F-Statistic

The F-statistic is used in the `aov()` function to compare means between multiple groups by finding the ratio of the variance across the group means and the variance of the residual error.  In other words, the F-statistic compares the variance between groups to the variance within groups.  The F-statistic is calculated by taking the mean squares of the treatment (i.e., the group means) and dividing by the mean squares of the error (i.e., the residuals).  The resulting statistic is compared to an F-distribution to determine the statistical significance.

```{r standard-aov-ftest}
# Use the standard aov() function 
aov_test <- aov(Time ~ Page, data = four_sessions)

# Print the summary
summary(aov_test)
```

The output gives several useful metrics that describe how well the data fit the linear regression model.  For the `Page` variable, the `Sum Sq` field in the output above gives the sums of squares from the regression model, or the difference between each page mean and the grand mean (i.e., the mean calculated from all observations combined).  The sums of squares of the residuals is the squared difference between each observation and the `Page` mean.  The F-statistic is constructed from the mean square of the treatment (i.e., `Page`) divided by the mean square of the residuals.

## Chi-Square Test

The chi-square test is used to evaluate observed count data and how it compares to an expected distribution.  Specifically, we are dealing with contingency tables and assessing if it is reasonable to conclude independence among variables (e.g., some treatment effect and outcome).  If we have an R x C contingency table, we can generate a null hypothesis to compare to our empirical data to evaluate if it's reasonable to conclude that the variables are independent.  For example, let's look at the `click_rates` data, which shows the number of clicks versus no-clicks with three different headlines.  

```{r click-rates-data-eda}
# Import click rates data
click_rates <- read_csv("../data/click_rates.csv")

# Print a table of the data
click_rates %>% 
  pivot_wider(names_from = "Headline", 
              values_from = "Rate") %>% 
  kbl(caption = "Click rates for three different advertising headlines") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()
```

Next, we want to form our null hypothesis, or the expected value, which in this case, would be that the number of clicks versus no-clicks (i.e., the click rate) is the same for each headline.  To do this, we want to calculate the sum of all of the clicks (14 + 8 + 12 = 34), calculate the sum of all of the no-clicks plus the total number of clicks (986 + 992 + 988 = 2,966 + 34 = 3,000), then calculate the overall click rate (34 / 3,000 = 0.01133).  So, for each headline then, we're looking at 11.33 clicks and 988.67 no-clicks (i.e., 1,000 - 11.33 = 988.67).  This is our expected distribution, or the expected number of clicks versus no-clicks for each headline.

The next thing we want to do is calculate the Pearson's residual, or the difference between the observed value and the expected value divided by the square root of the expected value, for each cell, or each click versus no-click observation.  We can accomplish this with a simple `case_when()` statement inside of a `mutate()` call, as I've done below.  The numbers are slightly different than those presented in the textbook, likely because of slight differences in how the authors calculated their residuals.   

```{r click-data-residuals}
# Calculate click data residuals
click_rates_residuals <- click_rates %>% 
  mutate(Expected_Rate = 
           case_when(Click == "Click" ~ as.numeric(11.67), 
                     TRUE ~ as.numeric(988.67)),
         Residual = (Rate - Expected_Rate) / sqrt(Expected_Rate), 
         Residual = round(Residual, digits = 3))

# Print the summary table with the residuals
click_rates_residuals %>% 
  select(Headline, Click, Residual) %>% 
  pivot_wider(names_from = "Headline", 
               values_from = "Residual") %>% 
  kbl(caption = "Pearson residuals for each headline") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()

```

The chi-squared test statistic is constructed by summing the squared Pearson residuals.  We can square all of the Pearson residuals and add them to calculate the chi-squared statistic below.

```{r calculate-chi-squared}
# Calculate chi-squared statistic by squaring all of the Pearson residuals, then summing the squared values
click_rates_residuals %>% 
  mutate(Squared_residual = Residual^2) %>% 
  summarise(Chi_sq = sum(Squared_residual))
```
To evaluate how likely it is to see a chi-squared test statistic estimate of 1.65 with data that were collected in another sample, we would perform a permutation test.  We would hold the marginal totals constant- in other words, the number of clicks and no-clicks would be constant (34 and 2,966, respectively), then take three samples each of 1,000 observations and count the number of clicks.  We'd compute the chi-squared statistic for the new permuted data, perform the permutation 1,000 times, then compare the original estimate (1.65) to the distribution of the chi-squared statistic from the permuted data to calculate a p-value.  We can accomplish all of this using the `chisq.test()` function and set the argument `simulate.p.value` to `TRUE`.  The first step is to transform the `click_rates` data frame into a matrix, setting the number of rows to 2, the number of columns to 3, and setting `byrow = TRUE`.  Finally, we can set the argument `B` (the number of replicates used) to 10,000.

```{r simulate-p-value-chisq}
# Use the chisq.test() function to calculate the chi-squared statistic and calculate the p-value
matrix(click_rates$Rate, 
       nrow = 3, 
       ncol = 2, 
       byrow = TRUE) %>% 
  chisq.test(simulate.p.value = TRUE, 
             B = 10000)

```
Based on the results above, we would conclude that the variation observed between headlines could reasonably be due to chance (p-value ~ 0.48).  

## Fisher's Exact Test

In cases where cell counts are less than 5, the assumption that the test statistic approximates the chi-squared distribution breaks down.  In these cases, it is preferred to use Fisher's Exact Test, which calculates all of the possible permutations, holding the row and column totals constant, in order to evaluate how extreme the empirical result is given all of the other possible combinations that could have been drawn from the same underlying data.  This can be calculated using the `fisher.text()` function.

```{r fishers-test}
# Perform Fisher's exact test on the same data
matrix(click_rates$Rate, 
       nrow = 3, 
       ncol = 2,
       byrow = TRUE) %>% 
  fisher.test()
```

This exact p-value, calculated by constructing all of the possible permutations, calculating the chi-squared statistic, and then comparing the number of estimates that were as extreme or more extreme than the one calculated from the empirical data, shows that the variation observed can reasonably be due to chance.  We can see another application using the `imanishi_data` containing a frequency table of the interior digits (ignoring the first and last digit of a number) in laboratory data.  The case rested under the assumption that under a uniform distribution, each digit would occur 31.5 times (315 total observations / 10 numbers = 31.5).  The empirical data are plotted below side-by-side with the null distribution.

```{r lab-data-eda}
# Import the imanishi data
lab_data <- read_csv("../data/imanishi_data.csv")

# Add a separate column for the expected counts for each digit
lab_data %>%
  mutate(Expected = 31.5) %>% 
  pivot_longer(cols = c(Frequency, Expected), 
               names_to = "Distribution", 
               values_to = "Count") %>% 
  mutate(Digit = as.factor(Digit)) %>% 
  ggplot(aes(Digit, Count, fill = Distribution)) +
  geom_bar(stat = "identity", 
           position = "dodge") +
  my_theme +
  scale_y_continuous(expand = expansion(mult = c(0.01, 0.01)))
  
```

From the bar plot above, we can see that the empirical distribution is quite different from the expected distribution, assuming all of the interior digits were drawn from a uniform random distribution.  Let's test this using a chi-squared test and performing 10,000 replicate resamplings.

```{r lab-data-chisq-test}
# Add a column of the expected counts for each interior digit and convert to a matrix
lab_data %>% 
  chisq.test(.$Frequency, 
             simulate.p.value = TRUE, 
             B = 10000)
  
```

In summary, chi-squared and Fisher's exact tests are used in data science to test if variables are independent (e.g., clicks based on a certain webpage layout). 

## Multi-Arm Bandit Algorithm

Multi-arm bandit algorithms are similar to A/B testing, but instead of setting a pre-defined threshold at which you'll stop the experiment, evaluate the results, and then immediately implement those results (e.g., change a webpage based on the results of the A/B test), the multi-arm bandit approach favors outcomes that begin to do better over time.  For example, if there are three webpage layouts being compared as in previous examples, and A appears to be "outperforming" B and C (i.e., getting more clicks), then the multi-arm bandit algorithm will start to feed users more of the A webpage, but not ignore B and C in case they start to get more clicks.  This approach is often used in web testing because it can allow the user to reach the overall conclusion more quickly than traditional A/B tests, and this approach utilizes information from previous experiments by changing the probability of which webpage is shown (in this example).  

One example of the algorithm is the epsilon-greedy algorithm, which is controlled by a single parameter, epsilon.  The pre-defined value of epsilon controls the behavior of the algorithm and in the context of webpage testing, which webpage a user is presented with (e.g., A or B).  Put simply, if a randomly drawn number is between 0 and epsilon, a coin is shown a if the user flipped heads, they are presented with A and if tails, they're presented with B.  If, however, the randomly drawn number is greater than epsilon, the algorithm shows the user the webpage with the highest number of clicks at that stage.  In this way, we can see that if one webpage has a higher response rate than the other, it's chosen preferentially to present to the user.  

## Statistical Power

A key consideration when designing experiments is defining the length of the experiment.  The central goal is to detect an effect, if one is present, but whether or not the effect has a dramatic effect on two groups will dictate how much data need to be acquired to detect that effect.  This is the statistical power, or the probability of detecting an effect size (minimum size of the effect the experiment aims to detect) given the sample size and variability within the sample.  In the example in the textbook, if we wanted to detect the difference between two types of baseball hitters in 25 at-bats, and we know that in 25 at-bats, the probability of seeing a difference is 0.75, the effect size is the difference between the baseball hitters, and the power is 75%. 

## Sample Size

In some cases when evaluating the appropriate sample size to use for a given hypothesis test, you'll need to define the effect size ahead of time.  In the example from the textbook, perhaps we are interested in running a new ad, but want to know if it's better than the one currently being used.  Further, we'll only switch over to the new ad if it generates 10% more responses than the ad currently being used.  The 10% is the effect size.  

Following the example from the book, we can calculate the sample size necessary in a webpage experiment.  We are interested in comparing two different ads, one ad that is already in production that has an average click-rate of 1.1%.  We want that click-rate to increase to 1.21% (i.e., a 10% increase), or else we'll keep the same ad, but we don't know how long to run the experiment for (i.e., how many times to show users ads) in order to detect the effect size.  We'll use the function `pwr.2p.test()` to calculate the sample size given the effect size and desired level of statistical power.  In the example from the book, we're striving for 80% power, or the ability to reject the null hypothesis 80% of the time.

```{r calculate-sample-size}
# Define the effect size (10%)
effect_size <- ES.h(p1 = 0.0121, 
                    p2 = 0.011)

# Calculate the sample size
pwr.2p.test(h = effect_size, 
            sig.level = 0.05, 
            power = 0.8, 
            alternative = "greater")
```
Let's change this slightly and see if we were interested in a higher statistical power (95%).

```{r calculate-alt-sample-size}
# Calculate sample size with greater power
pwr.2p.test(h = effect_size, 
            sig.level = 0.05, 
            power = 0.95, 
            alternative = "greater")

```

We can see that if we are striving for 80% power to detect an effect, we'll need around 116,000 samples; if we want to strive for higher statisticl power at 95%, we'll need almost 20,500 samples.  