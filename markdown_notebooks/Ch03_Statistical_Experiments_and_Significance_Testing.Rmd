---
title: "Chapter 3: Statistical Experiments and Significance Testing"
output:
  html_notebook:
    toc: true
    toc_float: true
    theme: cerulean
    highlight: tango
---

```{r setup, warning = FALSE, message = FALSE}
# Load libraries
library(tidyverse)
library(kableExtra)
library(ggtext)
library(RColorBrewer)
library(modelr)

# Set ggplot2 theme
my_theme <- theme(
  panel.background = element_rect(fill = "white", color = "black"), 
  panel.grid = element_blank(),
  plot.title = element_markdown(),
  plot.subtitle = element_markdown(),
  plot.caption = element_markdown()
)

```



## Introduction 

The basic workflow in any statistical analysis is to first formulate a hypothesis, design an experiment to test the hypothesis, collect data, analyze the data, and make conclusions and inferences inferences to the larger population that the sample was drawn from.

## A/B Testing

A/B testing involves the randomized allocation of subjects to treatment groups (those subjects that receive the treatment) and control groups (those subjects that don't receive the treatment) and evaluating a test statistic to measure the effects of the treatment.  For example, physicians may be interested in the effectiveness of a drug in treating a disease relative to a placebo, so subjects may be randomly assigned into treatment and control (i.e., placebo) groups to evaluate the drug's effectiveness. In this particular case of a clinical trial, blind or double-blind study designs may be used.  In a blind study design, the subjects are unaware if they were randomly assigned into the treatment or the control group; in a double-blind study design, both the subjects AND those administering the treatment are unaware which group the subjects have been placed in.  In data science, the A/B tests are typically motivated by business interests: evaluating which web page style gets more views; which site headline gets more clicks; or which product price gets more purchases.  Importantly, the researcher must establish the test statistic to measure the effect of the treatment prior to the experiment.

## Hypothesis Tests

Hypothesis tests are critical to evaluate the differences between true effects due to a treatment or similar, or natural random variation.  The idea behind random assignment in A/B tests is that any differences observed between treatment groups can be due to only two things: a) the random change in assignments of subjects to either the experimental or control group; or b) a true effect due to the treatment.  The hypothesis test relies on two hypotheses: the null hypothesis, which assumes that both groups are equal, and that any variation is due to chance; and the alternative hypothesis, which states that both groups are not equal, and that any variation cannot be reasonably attributed to chance alone.  One way in which the null hypothesis is framed is by creating a null distribution, by conducting a permutation test to shuffle subjects between two groups such that the permuted group sizes are the same as the original group sizes.  This is repeated a defined number of times, creating a distribution where there are no differences between groups.  This distribution can then be compared to the empirical data.  

Hypothesis tests can be one-tailed, or two-tailed, depending on the research question.  One-tailed tests only consider a single alternative, while two-tailed tests consider two alternatives.  For example, a typical one-tailed test in data science may involve comparing the current web page headline to a new, flashier web page headline.  In this situation, we may only be interested in whether or not the new web page design attracts more viewers, and if it doesn't, we'll just stick with the current design.  In a different situation, maybe we're starting from scratch and want to know which web page headline background color attacts more viewers: blue, or orange.  In this case, the orange background could attract more viewers, or it could attract less. 

## Resampling

In hypothesis testing, there are two forms of resampling, or redrawing samples from the observed data: bootstrapping, or resampling with replacement, typically to evaluate the variability in an estimate; and permutation, typically involving two or more groups, where observations may be randomly shuffled between groups to generate a null distribution (i.e., a null hypothesis of no difference between groups).  One of the key differences is in permutation testing, there is more than one group being compared, and we're going to be resampling _without_ replacement, because we're shuffling observations between groups to simulate the null hypothesis of no differences between groups.  Another key theme with the permutation test is that the original sample sizez will be maintained in each permutation.  For example, if you want to compare the group means between groups A and B, and there are 50 observations in group A and 48 observations in group B, there will be 50 and 48 observations in the permuted groups A and B, respectively, in each iteration.  With each iteration, the original estimate of the statistic will be recorded to generate a null distribution to compare to the observed value from the original, non-permuted data.  In essence, if the estimate calculated from the empirical distribution is within the null distribution created from the permutations, we may conclude that the estimate could be obtained simply by chance.  However, if the estimate is extreme and lies in one of the tails of the distribution, we may conclude that the chances that the estimate was obtained by chance are rare.  

To look at an example of the permutation test in action, we'll look at the `web_page_data.csv` file, which contains Google Analytics data on the amount of time users spent on a particular experimental web page.  The idea is to identify which web design users spent the most time on to drive business decisions for the hypothetical company.  

```{r google-analytics-sessions-plot}
# Load the four sessiosn data and convert the minutes to seconds
session_times <- read_csv("../data/web_page_data.csv") %>% 
  mutate(Time = Time * 100)

# Create boxplot of the page sessions 
session_times %>% 
  ggplot(aes(Page, Time)) +
  geom_boxplot() +
  my_theme +
  labs(y = "Time (seconds)", 
       title = "Time spent on different web pages",
       subtitle = "Data are from Google Analytics")
```

From this plot, we can see that Page B has the highest median time.  We can also plot the distribution of time spent on each webpage as a density plot and label the mean time spent as a vertical dashed line to visualize differences in means.

```{r google-analytics-sessions-density}
# Plot the distribution of times spent on each webpage and label the means for each page
session_times %>%
  ggplot(aes(Time)) +
  geom_density(aes(fill = Page), 
               alpha = 0.6) +
  geom_vline(data = filter(session_times, Page == "Page A"), aes(xintercept = mean(Time)), linetype = "dashed") +
  geom_vline(data = filter(session_times, Page == "Page B"), aes(xintercept = mean(Time)), linetype = "dashed") +
  my_theme +
  scale_x_continuous(expand = expansion(mult = c(0, 0))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  facet_grid(Page ~ .) +
  theme(
    legend.position = "none"
  ) +
  labs(x = "Time (seconds)", 
       y = "Density", 
       title = "Distribution of times spent on each webpage from Google Analytics data", 
       subtitle = "The mean time spent on each webpage is represented as a dashed line")

```

From this faceted plot, we can see users spent the least amount of time on Page A and the greatest amount of time on Page B.  Let's create a separate data frame to show a table of the mean time spent on each page. 

```{r session-times-table}
# Create a new data frame with just the mean times for each page
mean_session_times <- session_times %>% 
  group_by(Page) %>% 
  summarise(mean_times = mean(Time), 
            n = n())
  
# Display a table of all the mean times per page
mean_session_times %>% 
  kbl(col.names = c("Page", "Mean time", "n"), 
      caption = "Mean time spent on each webpage (data from Google Analytics)") %>% 
  kable_classic_2() %>% 
  kable_styling(full_width = FALSE, 
                "striped")

```

Looking at Page A and Page B, we can see that the difference in mean times spent on the different webpages is about 36 seconds.  Ultimately, we want to know if this difference can be attributed to differences in the webpages, or just random chance.  To do this, we'll create a permutation function (derived from the the same function in the textbook) that will take three arguments: `x`, the data frame to use; `n_A`, the number of samples in group A (e.g., Page 1); and `n_B`, the number of samples in group B.  We'll then create indexes to use in the permutation by randomly sampling numbers from 1 to the `n` (the total number of samples in the dataset) of the size of group B.  After that, we'll get the difference and assign those indexes to group A.  Then, we'll subset each group by randomly shuffling the indexes and calculate the mean for each group.  

```{r permutation-function}
# Create the permutation function
perm_function <- function(x, n_A, n_B) {
  n <- n_A + n_B
  index_B <- sample(1:n, n_B)
  index_A <- setdiff(1:n, index_B)
  mean_diff <- mean(x[index_B]) - mean(x[index_A])
  return(mean_diff)
}

```

After we've created the function, we'll perform 10,000 iterations to generate a null distribution of the data.

```{r permutation-reps}
# Create an empty data frame
perm_diffs <- rep(0, 1000)

# For loop to run the function 10,000 times
for (i in 1:1000) {
  perm_diffs[i] <- perm_function(session_times$Time, 21, 15)
}

# Plot the distribution and our empirical difference in means as a dashed line 
data.frame(permuted_mean_diff = perm_diffs) %>% 
  ggplot(aes(permuted_mean_diff)) +
  geom_density(fill = "lightskyblue", 
               alpha = 0.6) +
  geom_vline(xintercept = 36, 
            linetype = "dashed") +
  my_theme +
  scale_x_continuous(expand = expansion(mult = c(0, 0))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.01))) +
  labs(x = "Mean time difference (seconds)", 
       y = "Density", 
       title = "Distribution of mean time difference (seconds) from permutation test", 
       subtitle = "Mean difference from empirical data is represented as a dashed line")
```

We can now look at the percentage of the mean differences calculated from the permuted data that are as extreme or more extreme than what we observed from the empirical data.

```{r percentage-extreme}
# Calculate the percentage of as extreme or more extreme mean differences
data.frame(permuted_mean_diffs = perm_diffs) %>% 
  filter(permuted_mean_diffs >= 36) %>% 
  nrow()
```

There are 141 mean differences from the permutation data that are as extreme or more extreme than the difference calculated from the empirical data, or about 14%.  This is the p-value, which indicates that the result is not statistically significant.  The permutation test is relatively straightforward and doesn't rely on assumptions of normality, and is thus widely applicable to a variety of datasets.  

## Statistical Significance and p-Values

When conducting hypothesis tests, we want to know the probability that we would obtain the estimate of a particular statistic, if the null hypothesis were true (i.e., no difference between groups).  This, in essence, is the p-value, and it gives us an indication about the unusualness of a particular result, or the chances of observing that particular result by chance if the null hypothesis were true.  We can reuse the permutation function from the section above and apply it to the new example dataset looking at the number of conversions from a table of ecommerce data.  

```{r e-commerce-permutation}
# Generate a vector of the observed percentage of differences between B and A
obs_pct_diff <- 100 * (200 / 23539 - 182 / 22588) # 0.04391621

# Create a vector to store the permutation indexes
# We'll need the total number of non-conversions (45945) and the total number of successful conversions (200 + 182 = 382)
conversion <- c(rep(0, 45945), rep(1, 382))

# Create a vector to store the permutation results
perm_diffs <- rep(0, 1000) # store 1,000 0's....

# Run the function 1,000 times
for (i in 1:1000) {
  perm_diffs[i] <- 100 * perm_function(conversion, 23739, 22588)
}

# Visualize the null distribution 
data.frame(perm_diffs = perm_diffs) %>% 
  ggplot(aes(perm_diffs)) +
  geom_density(fill = "lightskyblue", 
               alpha = 0.6) +
  geom_vline(xintercept = obs_pct_diff, 
             linetype = "dashed") +
  my_theme + 
  scale_x_continuous(expand = expansion(mult = c(0, 0))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.02))) +
  labs(x = "Permuted conversion rate (%)", 
       y = "Density",
       title = "Null distribution of conversion rate (%) between webpages B and A", 
       subtitle = "The observed difference is denoted as a dashed line")
```

Looking at the plot above, it appears to be in the range that's plausible just by chance alone.  We can formalize this conclusion by calculating the p-value.  We'll take the number of observations from the null distribution that are as extreme or more extreme than the one we've observed and divide by the total number of observations in the null distribution (1,000).

```{r calculate-p-value}
# Calculate the p-value by hand
data.frame(perm_diffs) %>% 
  filter(perm_diffs >= obs_pct_diff) %>% 
  nrow() / 1000
```

The p-value is 0.298, meaning we would expect to see a conversion rate as extreme or more extreme than the one we observed about 30% of the time, if the null hypothesis was true.  Since the data are binary (conversion or no conversion), we can also perform a test for equal proportions under the binomial distribution using the `prop.test()` function.

```{r prop-test}
# Perform a test of equal proportions 
prop.test(x = c(200, 182), 
          n = c(23539, 22406), 
          alternative = "greater")

```

From this output we can see the proportion of successes (i.e., conversions, defined in the `x` argument) for each group, as well as the point estimate and 95% confidence interval for the chi-squared test statistic.  The p-value for this test is similar to that obtained from the permutation test and doesn't change the overall interpretation of the results.  

There are two errors commonly discussed in hypothesis testing: Type I errors, or false-positives, in which the researcher rejects the null hypothesis when the null hypothesis is true; and Type II errors, or false negatives, in which the researcher fails to reject the null hypothesis when the null hypothesis is false.  

## t-Tests

