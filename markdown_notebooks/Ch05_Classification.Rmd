---
title: 'Chapter 5: Classification'
output:
  html_notebook:
    toc: yes
    toc_float: yes
    theme: cerulean
    highlight: tango
---

```{r setup, warning = FALSE, message = FALSE}
# Libraries
library(tidyverse)
library(kableExtra)
library(modelr)
library(klaR)
library(ggtext)
library(MASS) # linear discriminant analysis
library(viridis)
library(broom)
library(mgcv)
library(ggtext)

# Set ggplot2 theme
my_theme <- theme(
  panel.grid = element_blank(), 
  panel.background = element_rect(fill = "white", color = "black"), 
  plot.title = element_markdown(), 
  plot.subtitle = element_markdown(), 
  plot.caption = element_markdown()
)

# Set kableExtra printing layout
kable_layout <- function(df, caption) {
  df %>% 
    kbl(caption = caption) %>% 
    kable_styling(bootstrap_options = "striped", full_width = FALSE) %>% 
    kable_classic_2()
}
```

## Learning Objectives and Notes

### Meeting 7/26

Learning Objectives:

* Describe a general approach to binary classification (tree models can be used for predicting multiple classes).

* Use naive Bayes to a binary categorical response from categorical predictors.

* Use linear discriminant analysis (LDA) to predict a binary categorical variable from normally distributed continuous or categorical predictors.

* Use logistic regression to predict a binary categorical response from predictors.  

Notes: 

* Bayes theorem calculates the probability of A given B, if you know the probability of B given A, and the probability of A, and the probability of B.

* Bayes theorem: $prob(true | class) = \frac{prob(class|true)prob(true)}{prob(class)}$

* For naive Bayes, it's important to note that it's generally used with categorical predictors.  If dealing with numeric predictors, you could bin observations, but that could introduce bias.  In other cases, predictors may already be binned (e.g., ages).

* Naive Bayes is "naive" because the predictors are assumed to be independent, which is not the case most of the time in real-world scenarios.

* LDA essentially takes covariance matrices and finds coefficients for predictors which maximizes the ratio of the sums of squares between groups and the sums of squares within groups.  

* Logistic regression uses a transformation to shrink continuous responses in linear regression to be between 0 and 1.  Additionally, instead of describing the mean response with a one unit change in the predictor, we describe the log-odds with a one unit change in the predictor.  We can use the logistic function to re-write the log-odds as a probability of assigning to a particular class.

## Introduction

* Classification is a form of supervised machine learning in which we use training data with defined binary outcomes to try and predict the outcome of new data (e.g., is an image a cat or a dog, or is an email normal or spam).

* In other cases, there are multiple "classes" in which a record could belong to, and tools can provide a probability of classification with thresholds used to determine the ultimate decision.

* In cases with multiple "classes," the problem can be broken up into several binary decisions: for example, 0 or 1, and then if 1, is an observation 1 or 2.  

* The basic workflow for these classification problems is as follows: establish a probability threshold, above which a record is assigned for a given class; estimate the probability that a record belongs to various classes; if the estimated probability is greater than the established threshold, assign the record to that class.

## Naive Bayes

* The basic tenant of the naive Bayes algorithm is to estimate the probability of observing a predictor value given the outcome (i.e., using the data themselves).  

* This is defined as the conditional probability, written as $P(Y_i|X_i)$, or the probability ($P$) of an outcome ($Y_i$) given some variable $X_i$.

* One of the major assumptions of the naive Bayes approach is that features (variables) are independent, and the presence of one feature doesn't influence the presence of another.  Another assumption made is that each feature has an equal effect on the outcome.  

* Using the same notation as above, we can write Bayes' theorem: $P(Y|X) = \frac{P(X|Y) * P(Y)}{P(X)}$

* Here, $X$ represents the different features used in the classification: $X = (X_1, X_2, X_3, ...,  X_n)$

* We can expand this expression using the chain rule to describe the probability of $Y$ for each of $X_n$ features:
$P(Y|X_1, X_2, X_3, ..., X_n) = \frac{P(X_1|Y)P(X_2|Y)P(X_3|Y)...P(X_n|Y)}{P(X_1)P(X_2)P(X_3)...P(X_n)}$

* In another example using [this StatQuest video](https://www.youtube.com/watch?v=O2L2Uv9pdDA), if we wanted to classify if an email was "normal" or "spam," we'd use the training data (i.e., the data for which we have known outcomes) to do two things: 1) estimate the probability that an email is "normal" or "spam" by taking the "normal" emails divided by the total (and similarly for the probability that an email is "spam"); and 2) calculating the probability that each word is observed in a "normal" versus "spam" email, by taking the number of times that word is observed in each respective class, divided by the total number of words in that class.  

* Using a specific example, we know the probability that an email is "normal" is 0.67 (8 emails are "normal" and there are 14 total emails), and the probabilities that the words "dear" and "friend" will be seen in a "normal" email are 0.47 and 0.29, respectively.  So, if we receive an email with the sentence "Dear friend," we essentially get rid of the denominator portion in Bayes' theorem to yield: $P(Y|X) = P(X|Y) * P(Y)$.

* In the context of the above example, $P(Y|X) = P(normal|"dear") * P(normal|"friend") * P(normal)$.  We can plug in each of these numbers to get the following: 0.47 (probability of seeing "dear" given that an email is "normal") x 0.29 (probability of seeing "friend" given that an email is "normal") x 0.67 (probability that the email is "normal"), which works out to 0.09.  We do the same thing for the other scenario, evaluating the probability of seeing each of those words if an email was "spam," which works out to 0.01.  These results are proportional to the probability that the email is of one of two classes, given the data.  In this example, 0.09 is greater than 0.01, so we would conclude that an email with "Dear friend" is a "normal" email.

* However, one of the pitfalls of naive Bayes is that it assumes each predictor is independent and ignores any complex relationships that may be present in the data.  In the above examples, there is no distinction made between "Dear friend" versus "Friend dear," even though the first configuration will have more weight than the second one.  

* Because of the inherent properties of the naive Bayes algorithm, it tends to have high bias.  Bias refers to the propensity of a model to over-simplify the relationship between the response and the predictor, measured as the difference between the mean predicted value and the actual value.  On the other hand, variance refers to the propensity of the model predictions to change when presented with new data, which is often a consequence of overfitting when a model captures the noise present in the data.

* In the example below, we'll estimate a naive Bayes model with the `loan_data`.  A subset of the larger data frame is shown below, which comprises data on the loan amount, the annual income of the borrower, the purpose of the loan, and the status of the loan. 

```{r loan-data-import, message = FALSE, warning = FALSE}
# Import data and convert all character fields to factors
loan_data <- read_csv("../data/loan_data.csv") %>% 
  rename("record" = X1) %>% 
  mutate(across(where(is.character), factor))

head(loan_data) %>% 
  kable_layout(caption = "Subset of loan data")
```

* We'll use a naive Bayes model to classify the loan outcome based on the purpose of the loan, whether or not the borrower owns or rents a home, and length of employment of the borrower.  First, we'll look at the number of different outcomes in this data set.

```{r loan-data-outcome-summary}
# Get summary information on the loan outcomes
loan_data %>% 
  group_by(outcome) %>% 
  summarise(count = n()) %>% 
  kable_layout(caption = "Loan outcome data")
```

* Now, we'll fit the naive Bayes model to classify the loans as either `default` (failure to meet the legal obligations of the loan), or `paid off`.  

```{r loan-data-naive-bayes}
# Fit the naive Bayes model
loan_naive_bayes <- NaiveBayes(outcome ~ purpose_ + home_ + emp_len_, 
                               data = na.omit(loan_data))

# Print model output
loan_naive_bayes$tables
```

* The model results are proportional to $P(X|Y)$, or the conditional probability of some feature, given the outcome.  For example, if we look at the `purpose_` field, we can see the conditional probabilities of debt consolidation for the default class versus the paid-off class.  Thus, if we wanted to use these results to predict the loan status of a new credit card loan for a borrower that owns a home and has been employed for more than one year, the equation would look like this (for one outcome): $P(Y|X) = P(default|creditcard) * P(default|own) * P(default|>1 year) * P(default)$, which ultimately works out to $P(Y|X) = 0.15151515 * 0.0832782 * 0.95271492 * 0.5 = 0.006010635$.

* We get the a priori probabilities from the `apriori` slot in the `loan_naive_bayes` object, which is simply the probability of a `default` or `paid off` outcome, given the number in each category and the total number of loans in the data set.  We can compare the above probability of a default loan, compared to a paid off loan, by substituting the other conditional probabilities: $P(Y|X) = P(paidoff|creditcard) * P(paidoff|own) * P(paidoff|>1 year) * P(paidoff)$, which works out to $P(Y|X) = 0.18759649 * 0.0808963 * 0.96894711 * 0.5 = 0.007352304$. 

* Based on these two results, we would conclude that a borrower that owns a home and has been employed for more than 1 year will pay off a credit card loan. 

```{r naive-bayes-predictions}
# Predict posterior probability of a record using the naive Bayes classifier
new_loan <- loan_data %>% 
  filter(purpose_ == "credit_card" & home_ == "OWN" & emp_len_ == "> 1 Year") %>% 
  dplyr::select(purpose_, home_, emp_len_) %>% 
  head(n = 1)

predict(loan_naive_bayes, new_loan)
```

* We can take this one step further and classify the observations as `default` or `paid off` based solely on these predictors using the naive Bayes results.

```{r naive-bayes-predictions-all}
# Predict
loan_subset <- loan_data %>% 
  filter(purpose_ == "credit_card" & home_ == "OWN" & emp_len_ == "> 1 Year") %>% 
  dplyr::select(record, purpose_, home_, emp_len_, outcome) 
  
loan_classifications <- predict(loan_naive_bayes, loan_subset) %>% 
  pluck("class") %>% 
  data.frame(classification = .) %>% 
  cbind(loan_subset) 

# Plot to visualize the differences
loan_classifications %>% 
  mutate(assignment = ifelse(classification == outcome, "correct", "incorrect")) %>% 
  group_by(assignment) %>% 
  summarise(count = n()) %>% 
  mutate(count_prop = round(count / 536, 2)) %>% 
  kable_layout(caption = "Proportion of correct and incorrect loan outcome assignments using a naive Bayes classifier")
```

* We can see from this table that the naive Bayes classifier only correctly assigned records about 54% of the time, indicating there are likely other features that are important to categorize outcomes.  

* [This](https://uc-r.github.io/naive_bayes) is another good tutorial for implementing naive Bayes models using other packages like `caret` and `h2o` and employing a cross-validation framework to evaluate the model performance.  

* One thing to note is that if a feature doesn't appear in a training set for the model for a particular response class, the posterior probability of the response will be 0, since we're multiplying conditional probabilities for each feature and response class.  To mitigate this problem, we can use a Laplace smoother, which adds a small number to the counts in each feature, such that the counts of each feature are non-zero.  

* We can apply this same naive Bayes algorithm to the `iris` data set, which is composed of 150 records and 5 variables including `Sepal.Length`, `Sepal.Width`, `Petal.Length`, `Petal.Width`, and `Species`.  

```{r naive-bayes-iris}
# Apply the naive Bayes classifier to the iris dataset 
iris_naive_bayes <- NaiveBayes(Species ~ ., data = iris)

# Predict the species using the classifier
iris_predictions <- predict(iris_naive_bayes, iris) %>% 
  pluck("class") %>% 
  data.frame(classification = .) %>% 
  cbind(iris) %>% 
  mutate(assignment = ifelse(classification == Species, "correct", "incorrect"))

# Print results
iris_predictions %>% 
  group_by(assignment) %>% 
  summarise(count = n()) %>% 
  mutate(count_prob = round((count / 150), 2)) %>% 
  kable_layout(caption = "Probability of correct iris species assignments using the naive Bayes classifier")
```

## Discriminant Analysis

* One of the most widely used discriminant analyses is the linear discriminant analysis (LDA), which applies a discriminant function to maximize the separation of records into distinct classes using the discriminant weights. 

* Covariance describes the joint relationship between two variables.  For example, if greater values of one variable are associated with greater values of another variable, the covariance between them is positive; the same holds true with lesser values of one variable and lesser values of another.  If greater values of one are associated with lesser values of another, the covariance is negative.  

* For example, using the `iris` data set again, we can find the covariance between each of the variables after removing the `Species` field.  Note, covariance is not constrained between -1 and 1, but depends on the scale of the variables being considered. 

```{r iris-covariance matrix}
iris %>% 
  dplyr::select(-Species) %>% 
  cov()
```

* Fisher's LDA focuses on distinguishing the variation between groups ($SS_{between}$, or the squared distance between the two group means) and the variation within groups ($SS_{within}$, or the spread around the mean within each group), trying to maximize the former and minimize the latter.  

* Specifically, LDA focuses on maximizing the sums of squares between groups to find a linear combination of features that maximizes the ability to distinguish between groups.

* We can use the `MASS::lda()` function with the `loan3000.csv` data to classify loan outcomes using the `borrower_score` and `payment_inc_ratio`.  The output will be the estimated linear discriminator weights, or essentially the coefficient used to estimate the probability that a record belongs to one class or another.

```{r lda-loan-data, message = FALSE}
# Apply LDA to load data to classify outcome using just the borrower score and the payment income ratio
loan3000 <- read_csv("../data/loan3000.csv") %>% 
  rename("record" = X1) %>% 
  mutate(across(where(is.character), factor)) 

loan_lda <- lda(outcome ~ borrower_score + payment_inc_ratio,
                data = loan3000)

loan_lda$scaling %>% 
  data.frame() %>% 
  rownames_to_column("Feature") %>% 
  kable_layout(caption = "Linear discriminator weights to predict loan outcome")
```

* We can then use the `predict()` function to use the linear discriminator weights to estimate the posterior probability that a loan will have the outcome `default` or `paid off`.

```{r lda-predict}
# Predict outcome of each loan using the LDA results
loan_lda_posteriors <- predict(loan_lda) %>% 
  pluck("posterior") %>% 
  data.frame() %>% 
  rownames_to_column("record")

# Print output
head(loan_lda_posteriors) %>% 
  kable_layout(caption = "Subset of posterior probabilities of loan outcomes using LDA")
```

* Now, we can plot the predicted values using the LDA results with `ggplot2` to see the separation of observations into different classes.  The linear discriminants, or the scores on each of the linear discriminant axis for each observation, are given in the `x` slot in the `predict()` output.

```{r lda-plot}
# Plot the values from the LDA
loan_lda_predictions <- predict(loan_lda) %>% 
  data.frame()

loan_lda_predictions %>% 
  ggplot(aes(LD1, fill = class)) +
  geom_density(alpha = 0.65) +
  scale_y_continuous(expand = expansion(mult = c(0.001, 0.05))) +
  my_theme +
  labs(x = "Linear discriminant 1", 
       y = "Density", 
       title = "Distribution of loan outcomes using linear discriminant analysis")

```

* From this density plot, we can see that the first linear discriminant performs reasonably well to distinguish loan outcomes, but there is a region of overlap where the discriminatory ability of the function declines.  

* We can take this one step further and construct a scatterplot with the borrower scores on the x-axis, the payment income ration on the y-axis, applying a color gradient corresponding to the probability of assigning to `default`, and then applying a solid line to visualize the LDA decision boundary.

```{r lda-decision-boundary-plot, warning = FALSE, message = FALSE}
# Prepare the data for the center of the plot, the slope of the line, and the intercept
center <- 0.5 * (loan_lda$means[1, ] + loan_lda$means[2, ]) # gets center for both features
slope <- -loan_lda$scaling[1]/loan_lda$scaling[2]
intercept <- center[2] - center[1] * slope

# Plot
loan3000_predictions <- loan_lda_predictions %>% 
  cbind(loan3000)

loan3000_predictions %>% 
  ggplot(aes(borrower_score, payment_inc_ratio, color = posterior.default)) +
  geom_point(alpha = 0.65) +
  scale_color_viridis("Probability of default", option = "plasma") +
  scale_y_continuous(expand = expansion(mult = c(0, 0)), 
                     lim = c(0, 20)) +
  geom_abline(slope = slope,
              intercept = intercept, 
              linetype = "dashed") +
  my_theme +
  labs(x = "Borrower score", 
       y = "Payment to income ratio")

```

* From this plot, we can see the decision boundary is the dashed line, where the probability of default is 0.50.  Warmer hues indicate a higher probability of assignment to default, while cooler hues indicate lower probability of default.  

* There are other approaches to discriminant analysis, and one of the best known is quadratic discriminant analysis (QDA).  The main difference between LDA and QDA is that the former assumes the covariance matrix is the same for both groups, while in the latter application, the covariance matrix can be different.

## Logistic Regression

### Logistic Response Function and the Logit

* Logistic regression is similar to simple and multiple linear regression, but instead of a continuous response, we're looking at a binary response.

* Ultimately, we're modeling the probability that an observation takes on "class" of binary response (e.g., "yes" or "no"), but we need the logistic response function or inverse logit function to transform the linear function such that the resulting probability is between 0 and 1. Put another way, it squeezes the output of a linear function to be between 0 and 1.

* The logistic function takes the form: $P(X) = \frac{e^{B_0+B_1X_1+...+B_nX_n}}{1 + e^{B_0 + B_1X_1 + ... + B_nX_n}}$

* This expression can be manipulated to get the odds, which is the probability of "success" divided by the probability of "failure:" $\frac{P(X)}{1-P(X)} = e^{B_0 + B_1X_1 + ... + B_nX_n}$

* The odds can take on any number from 0 to infinity, with higher values corresponding to higher probabilities of success relative to the probability of failure.

* We can take the logarithm of each side of the above expression to get the following: $log(\frac{P(X)}{1-P(X)}) = B_0 + B_1X_1 + ... + B_nX_n$, or the log-odds (also called the logit).  This equates to a linear relationship with $X$.

* When interpreting the linear regression model, each unit change in $X$ equates to a $B_n$ increase in the mean response, $Y$.  However, in logistic regression, each unit change in $X$ equates to a one unit increase in the log odds of $B_n$, which can be rewritten as $e^{B_n}$.

* We can take this one step further to use in classification by establishing a probability threshold, above which an observation will be grouped into a category (e.g., probability of 0.5).

### Logistic Regression and the GLM

* We can use the `glm()` function to fit a logistic regression model, since it is an extension of the GLM, but specifying `family = "binomial"` in the function call.  We'll use the loan data again to classify the outcome using the payment to income ratio, purpose of the loan, home ownership status, length of employment, and the borrower score.

```{r loan-data-logistic-regression}
# Fit a logistic regression model to the loan data
loan_logistic_mod <- glm(outcome ~ payment_inc_ratio + purpose_ + home_ + 
                           emp_len_ + borrower_score, 
                         data = loan_data, family = "binomial")

# Print the model results
data.frame(coefficient = loan_logistic_mod$coefficients) %>% 
  rownames_to_column("term") %>% 
  kable_layout(caption = "Coefficients from logistic regression modeling loan outcomes")
```

* Using these coefficients, we can predict the probability that a homeowner with a borrower score of 1.00 (the highest in the data set and is associated with the credit of the borrower) and a debt consolidation loan, a payment income ratio of 5, and has been employed for more than one year will not default on their loan using the expression: $P(default = yes) = \frac{e^{-1.6380924 + -0.0797366*1 + -0.2493727*1 + -0.0483297*1 + 0.3567310*1 + 4.6126378*1.00}}{1 + e^{-1.6380924 + -0.0797366*1 + -0.2493727*1 + -0.0483297*1 + 0.3567310*1 + 4.6126378*1.00}}$, or about 0.95.  

* Generalized linear models (GLMs) all have two main components: a probability distribution (in the case of the logistic regression, that's the binomial distribution to model binary responses), and a link function, to map the responses to to the predictors (the logit in the case of logistic regression).

* We can use the `predict()` method with the logistic regression model object, which will give the results in the log odds taking the form $log(\frac{P(default)}{1 - P(default)}$.  To get the probability, we simply apply the logistic function $P(X) = \frac{1}{1-e^n}$, where $n$ is the log-odds value.

```{r loan-logistic-predictions}
# Calculate the log-odds from the logistic model
loan_logistic_predictions <- predict(loan_logistic_mod)

# Calculate the probability of default from the log-odds (predictions)
loan_logistic_prob <- 1/(1 + exp(-loan_logistic_predictions))

# Put this back together with the original loan data
# We'll also classify each record as "default" or "paid-off" based on the probability > 0.5
loan_data_predictions <- data.frame(probability = loan_logistic_prob) %>% 
  cbind(loan_data) %>% 
  mutate(assignment = ifelse(probability > 0.5, "paid off", "default"))

# How many assignments were correct?
loan_data_predictions %>% 
  mutate(outcome_assignment = ifelse(assignment == outcome, "correct", "incorrect")) %>% 
  group_by(outcome_assignment) %>% 
  summarise(count = n()) 
```

* The logistic regression is popular in part because of its interpretability in terms of the odds.  Once we exponentiate the log-odds result from the model coefficient, we can interpret the odds as a multiplicative difference between two responses.  Because the untransformed model results are on the log-scale, a one unit increase equates to a $exp(1)$ increase in the odds ratio, or 2.72. 

### Fitting the model

* One of the major differences between linear regression and logistic regression is how the latter model is fit.  In linear regression, we use least squares to find the coefficients of the predictors and fit the model, then evaluate the model fit using the root mean squared error (RMSE).  This metric is simply the sum of all of the squared distances between the predicted values and the actual values, all divided by the number of observations.

* Unlike linear regression, in logistic regression, maximum likelihood is used to fit the model.  Maximum likelihood estimation (MLE) is essentially the process of identifying parameter values that are most likely to have produced the observed data.

* In the context of the logistic regression, MLE finds the solution with the log-odds that best fits the observed data.  

* In MLE, the set of parameters $\theta$ are chosen that maximize the probability of the data given the predictor values.  To estimate different logistic regression models or other models fitted using MLE, we can use the deviance metric, with lower deviance estimates corresponding to better model fit.

### Assessing the model

* Since the ultimate goal is to develop a model that can be used to classify new data, we can evaluate a logistic regression model by how accurate it is when making predictions.

```{r loan-logistic-regression-summary}
# Print summary of the logistic regression model
summary(loan_logistic_mod)
```

* All of the signs of the coefficients in the above results summary are opposite of the signs of the coefficients presented in the textbook, which indicates that the reference encoding is `default = 0` and `paid off = 1`.  For example, the `borrower_score` feature, which describes the borrowers credit worthiness, would be expected to increase the probability of paying off a loan, not defaulting.  

* Below, we'll fit a generalized additive model (GAM) with a spline term for the payment to income ratio and borrower score and evaluate the deviance values and AIC between models to see which model is a better fit.

```{r loan-logistic-gam}
# Fit a GAM with a spline term for payment to income ratio
loan_logistic_gam <- gam(outcome ~ s(payment_inc_ratio) + purpose_ + 
                           home_ + emp_len_ + s(borrower_score), 
                         data = loan_data, family = "binomial")
```

### Analysis of residuals

* We can calculate the partial residuals, or the residuals associated with each feature for the logistic regression model, just as we have done previously for the linear regression models.

```{r logistic-mod-partial-residuals}
# Calculate partial residuals
predictions <- predict(loan_logistic_gam, type = "terms") %>% 
  data.frame()

partial_resid <- resid(loan_logistic_gam) + predictions %>% 
  data.frame()

# Build data frame with just the payment to income ratio observed data, the predicted values, and the partial residuals, then plot
df <- data.frame(payment_inc_ratio = loan_data$payment_inc_ratio, 
           predictions = predictions$s.payment_inc_ratio., 
           partial_resid = partial_resid$s.payment_inc_ratio.)
df %>% 
  ggplot(aes(payment_inc_ratio, partial_resid)) +
  geom_point(alpha = 0.65) + 
  geom_line(aes(payment_inc_ratio, predictions), 
            linetype = "dashed", 
            color = "red") +
  my_theme + 
  labs(x = "Payment to income ratio", 
       y = "Partial residuals", 
       title = "Partial residuals plot from generalized additive model with spline term")
```

* Here, the points above the red line correspond to paid off loans, while the cloud of points below the red line correspond to defaulted loans.

## Evaluating Classification Models

* Often, training sets are constructed from the data, to train and fit a statistical model, which can then be applied to a new, unseen test set.  The accuracy of the model can then be evaluated based on how well it predicts the true outcomes in the test set.

* Accuracy is a common metric used, defined as the sum of the true positives and true negatives, divided by the sample size.

### Confusion Matrix

* A confusion matrix is a table showing the number of correct and incorrect predictions made by the classification model by the type of response. 

```{r logistic-gam-confusion-matrix}
# Calculate the predictions
predictions <- predict(loan_logistic_gam, newdata = loan_data)
predictions_y <- as.numeric(predictions > 0)
true_y <- as.numeric(loan_data$outcome == "paid off") # convert factor to numeric

# Put these into a data frame and assign classes
df <- data.frame(predictions_y = predictions_y, 
           true_y = true_y) %>% 
  mutate(class = case_when(
    true_y == 1 & predictions_y == 1 ~ "true_positive", 
    true_y == 0 & predictions_y == 0 ~ "true_negative", 
    true_y == 0 & predictions_y == 1 ~ "false_positive", 
    TRUE ~ "false_negative")
  )

# Summarize and generate confusion matrix
confusion_mat <- df %>% 
  group_by(class) %>% 
  summarise(count = n())

confusion_mat
```

* This case is slightly different than the textbook, but the numbers are still the same, just the interpretation is opposite.  Here, our model correctly classified 14,620 loans as paid off and 14,293 as defaulted.  However, it correctly classified 8,378 loans as paid off when they were default, and incorrectly classified 8,051 loans as default when they were actually paid off.  We can confirm this by merging `df` with the original `loan_data` and checking these numbers.

```{r check-confusion-matrix}
# Filter combined data to check that the interpretation is correct
cbind(df, loan_data) %>% 
  filter(class == "false_positive") %>% 
  group_by(outcome) %>% 
  summarise(count = n())
```

* In cases where there the outcomes are unbalanced, such as when a particular class is rare, we may not need to worry about identifying each of those rare cases.  Although it won't be as accurate (i.e., it may not be 100% accurate), it will generally correctly classify observations on new data.

### Precision, recall, and specificity

* __Precision:__ the sum of the true positives divided by the sum of all of the true positives and false negatives (i.e., those that were flagged as negative but were actually positive).  Written as an expression: $precision = \frac{true-positives}{true-positives + false-positives}$

* __Sensitivity:__ this metric evaluates the strength of the model to predict a positive outcome, or the proportion of the total positives that the model identifies.  For example, we would say that a model is very sensitive if it correctly identifies 10 out of 10 positive outcomes.  Written as an expression: $sensitivity = \frac{true-positives}{true-positives + false-negative}$

* __Specificity:__ this metric refers to the model's ability to identify a negative outcome.  Written as an expression: $specificity = \frac{true-negatives}{true-negatives + false-positives}$ 

* We can calculate each of these metrics from the confusion matrix generated above.

```{r calculate-metrics}
# Calculate other metrics from confusion matrix
diagnostic_metrics <- confusion_mat %>% 
  pivot_wider(names_from = "class", 
              values_from = "count") %>% 
  mutate(precision = true_positive / (true_positive + false_positive), 
         sensitivity = true_positive / (true_positive + false_negative), 
         specificity = true_negative / (true_negative + false_positive), 
         across(where(is.numeric), function(x) round(x, digits = 2)))

diagnostic_metrics %>% 
  kable_layout(caption = "Confusion matrix and diagnostic metrics from logistic regression model")
```

### ROC curve

* There is a trade-off between model sensitivity and specificity: the former measures how well the model classifies positive outcomes, while the latter captures how well the model classifies negative outcomes, ideally not classifying observations incorrectly.

* The receiver operating characteristics curve, or ROC curve, captures both of these metrics and plots the specificity on the x-axis and the sensitivity on the y-axis.  

```{r ROC-curve}
# Sort records by the predicted probability of being a 1, or paid off
index <- data.frame(log_odds = predictions) %>% 
  rownames_to_column("record") %>% 
  mutate(record = as.numeric(record)) %>% 
  arrange(desc(log_odds))

# Compute the cumulative specificity and sensitivity based on the sorted records
index %>% 
  left_join(loan_data, by = "record") %>% 
  mutate(sensitivity = cumsum(outcome == "paid off") / sum(outcome == "paid off"), 
            specificity = (sum(outcome == "default") - cumsum(outcome == "default")) / sum(outcome == "default"))
```

## Strategies for Imbalanced Data
