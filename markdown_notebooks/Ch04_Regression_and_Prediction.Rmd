---
title: "Chapter 4: Regression and Prediction"
output:
  html_notebook:
    toc: true
    toc_float: true
    theme: cerulean
    highlight: tango
---

```{r setup, warning = FALSE, message = FALSE}
# Load libraries
library(tidyverse)
library(forcats)
library(ggtext)
library(ggrepel)
library(scales)
library(janitor)
library(kableExtra)
library(broom)
library(patchwork)
library(lubridate)
library(MASS)
library(glmnet)
library(tidymodels)
library(modelr)
library(ggcorrplot)

# Set ggplot2 theme
my_theme <- theme(
  panel.background = element_rect(color = "black", fill = "white"), 
  panel.grid = element_blank(),
  plot.title = element_markdown(),
  plot.subtitle = element_markdown(),
  plot.caption = element_markdown(face = "italic")
)

# Set kable layout and take a caption as an argument
kable_layout <- function(df, caption) {
  kbl(df, caption = caption) %>% 
    kable_styling(full_width = FALSE, 
                  bootstrap_options = "striped") %>% 
    kable_classic_2()
}

```


# Introduction

This chapter focuses on prediction using regression, or identifying the quantitative relationship between a number of predictor variables and a response.  This is a form of supervised learning, in which we are training a statistical model with known outcomes in order to generalize to unknown, or unseen data, within the bounds of the predictor variables in the training data set.  We will also explore the subfield of anomaly detection using the relationship between the predictors and responses to identify responses that are extreme or unusual (e.g., outside the range of values that would be expected 95% of the time). 

## Simple Linear Regression

Notes:

* Regression is similar to correlation in that we're quantifying the relationship between one or more predictor variables and a response variable; the difference is that correlation measures the strength of the relationship (e.g., 0.99 correlation), while regression gives us the nature of the relationship (e.g., for every one unit increase in X, Y increases by 2.5 units).

* Importantly, the y-intercept is the value of the resonse variable when X = 0; in other words, this is the "default" or average baseline response (e.g., when X = 0, the y-intercept may be $150,000 for home prices in Seattle, meaning that without considering square footage or anything else, that's the "base price" to build off of). 

* The plot below shows the relationship between peak expiratory flow rate and exposure to cotton dust from the `LungDisease.csv` dataset (more information about the hazards of lung disease and obstructive dust from handling and processing cotton can be found on the [CDC website](https://www.labor.nc.gov/cotton-dust#hazard-overview)).

```{r lung-disease-eda}
# Import the data
lung_disease <- read_csv("../data/LungDisease.csv")

# Generate the scatterplot
lung_disease %>% 
  ggplot(aes(Exposure, PEFR)) +
  geom_point() +
  my_theme +
  labs(title = "Relationship between exposure to cotton dust and peak expiratory flow rate (PEFR)")
```

* The relationship is difficult to discern just looking at the scatterplot, so we can use the `lm()` function in R to construct a linear model and quantify the relationship between PEFR and exposure.

```{r regression-model-1}
# Create a linear regression model for exposure (predictor) and PEFR (response)
mod1 <- lm(PEFR ~ Exposure, data = lung_disease)

# Explore the summary information from the model output
summary(mod1) %>% 
  tidy() %>% 
  kbl(caption = "Coefficient estimates from a simple linear regression model between exposure (predictor) and PEFR (response)") %>% 
  kable_styling(full_width = FALSE,
                bootstrap_options = "striped") %>% 
  kable_classic_2()
```
* From this output, we can see that both the `Intercept` and `Exposure` terms are statistically significant and are informative in the model.

* When interpreting the coefficients, without taking into account any cotton dust exposure, the mean PEFR is approximately 424.

* When we take into account exposure, for every one unit increase in exposure to cotton dust, there is a 4 unit decrease in PEFR.  

* The residuals are calculated by subtracting each predicted value from the original value and serves as an indication of how well the model explains the variability in the response data.  

* We can add both the predictions and the residuals as fields in the original data frame using the `modelr` package (see below).

```{r add-predictions-residuals}
# Add predicted values and residuals to the original data frame
lung_disease_mod <- lung_disease %>% 
  add_predictions(model = mod1) %>% 
  add_residuals(model = mod1)

# Plot the predicted values and the original data
lung_pred_plot <- lung_disease_mod %>% 
  pivot_longer(cols = c("PEFR", "pred", "resid"), 
               names_to = "type", 
               values_to = "value") %>% 
  filter(type %in% c("PEFR", "pred")) %>% 
  ggplot(aes(Exposure, value)) +
  geom_point(aes(color = type), alpha = 0.75) +
  scale_color_manual(values = c("darkgrey", "red")) +
  my_theme +
  theme(legend.position = "none") +
  labs(y = "Value", 
       title = "Relationship between exposure to cotton dust and PEFR", 
       subtitle = "<strong style='color:darkgrey';>Original data</strong> are shown in grey; 
       <strong style='color:red';>predicted values</strong> are shown in red")

# Plot the residuals
lung_residuals_plot <- lung_disease_mod %>% 
  pivot_longer(cols = c("PEFR", "pred", "resid"), 
               names_to = "type", 
               values_to = "value") %>% 
  filter(type %in% c("resid")) %>% 
  ggplot(aes(Exposure, value)) +
  geom_point() +
  geom_hline(yintercept = 0, 
             linetype = "dashed", 
             color = "red") +
  my_theme +
  labs(y = "Value", 
       title = "Residuals from the linear model between exposure and PEFR")

# Put plots together
lung_pred_plot
```

* From the plot above, we can see the predicted values from the linear model shown in red (i.e., the mean response for each value of the predictor variable) and the original data shown in grey.

* There appears to be considerable variability in the mean response; we can dig into this more deeply looking at a plot of the residuals, or the difference between the predicted value and the original data.

```{r residuals-plot}
# Plot the residuals
lung_residuals_plot

```

* We can see that the dashed line at Y = 0 and ideally, if the linear model described the variation in the original data, most of the points would cluster around that dashed line.  We can see that especially at higher levels of exposure, there is considerable variability.

* The regression coefficients are chosen by minimizing the residual sum of squares (RSS), or coefficients which minimize the sum of the squared distances between the original data and the predicted value.

* Using linear regression to find coefficient estimates which minimize the RSS is called least squares regression or ordinary least squares regression.

## Multiple Linear Regression

Notes:

* The overall concepts are the same for multiple linear regression as they are for simple linear regression, except now we're dealing with more than one predictor.

* For example, we can look at the `house_sales.csv` data set, which contains data on the adjusted sale price for homes in King County, Washington, along with a number of different predictors including square feet of living space, square feet of the lot, the number of bedrooms and bathrooms in different fields, and the building grade.  Multiple linear regression can help us to see the relative contributions of each predictor in explaining the variability in the response.  Note, the data set needed to be reformatted prior to importing as the fields were shifted.

```{r king-county-housing-model}
# Import the king county dataset
house_sales <- read_csv("../data/house_sales.csv")

# Reformat the date field
house_sales <- house_sales %>% 
  mutate(DocumentDate = as_date(DocumentDate, format = "%m/%d/%y"))

# Isolate fields of interest
house_sales_subset <- house_sales %>% 
  dplyr::select(AdjSalePrice, SqFtTotLiving, SqFtLot, Bathrooms, Bedrooms, BldgGrade)

# Build linear model and drop records with missing data
house_lm <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + Bedrooms + BldgGrade, 
               data = house_sales_subset, 
               na.action = na.omit)

# Print model results
summary(house_lm) %>% 
  tidy() %>% 
  mutate(across(where(is.numeric), round)) %>% 
  kbl(caption = "Model results for housing data in King County, Washington (rounded to the nearest integer)") %>% 
  kable_styling(full_width = FALSE, 
                bootstrap_options = "striped") %>% 
  kable_classic_2()
```

* After constructing the multiple linear regression, we can see that all of the terms are statistically significant except the `SqFtLot` term, which defines the square feet of the lot (the p-value is greater than 0.05).  

* Not surprisingly, it looks like the terms with the greatest influence, or the greatest change in the response (adjusted sales price) with each change in their value, are the square feet of total living space and building grade.

* In the context of the data set, for every one square foot increase in total living space, the mean adjusted sales price rises $229.  

* We can get other information about the model using `glance()`.

```{r glance-house-model}
# Use glance to gather additional summary information about the model
glance(house_lm) %>% 
  kbl(caption = "Summary information from the model evaluating adjusted home sale price in King County, Washington") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()

```

* Looking at the R-squared and adjusted R-squared, the current predictors only explain a little over 50% of the variability in the response.  

* Cross-validation is used to set aside a training set (usually most of the data) to train the statistical model, and then apply it to a new, unseen set (the validation set).

* The adjusted R-squared adds a penalty term to the model in an effort to prevent overfitting; the AIC value also penalizes adding an additional terms.  Since the AIC method uses the logarithm of the residual sums of squares, a _k_ increase in the number of additional variables equates to a $2K$ penalty for the model.  

* Stepwise regression can be performed, either by adding in variables one at a time (forward selection), or starting with all variables and removing them one at a time (backward elimination) to evaluate how the different combinations influence model fit using the metrics defined above.  We can also use the `stepAIC()` in the `MASS` package to evaluate models based on the AIC value.

```{r stepwise-regression-AIC}
# Use the stepwise AIC method to identify the "best" model with a few additional fields
house_full <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + 
                   Bedrooms + BldgGrade + PropertyType + NbrLivingUnits +
                   SqFtFinBasement + YrBuilt + YrRenovated + NewConstruction, 
                 data = house_sales,
                 na.action = na.omit)

# Perform the stepwise regression 
stepwise_model <- stepAIC(house_full, 
                          direction = "both", 
                          trace = FALSE)

# Get the summary information from the ANOVA
stepwise_model$anova
```

* From the stepwise AIC selection tool, we can see the initial model with all of the variables, then the final model with the lowest AIC including fields for square feet of total living space, number of bedrooms and bathrooms, building grade, property type, square feet in the basement, and the year built (the coefficients for the model terms are presented below).

```{r stepwise-model-terms}
# Print the coefficients for the final model
stepwise_model %>% 
  tidy() %>% 
  kbl(caption = "Coefficients for the final model after performing stepwise regression using AIC") %>% 
  kable_styling(full_width = FALSE, 
                bootstrap_options = "striped") %>% 
  kable_classic_2()
```

* Other model selection methods like ridge regression or LASSO regression penalize the inclusion of too many parameters, effectively shrinking the coefficients towards zero such that only the parameters that explain a relatively large proportion of the variability (i.e., the most important terms) are included in the model.

* [This blogpost](https://drsimonj.svbtle.com/ridge-regression-with-glmnet) has a great tutorial for using ridge regression over OLS regression, which I've implemented on the housing data below using the same parameters as the stepwise AIC input.  One important component of the ridge regression model is the inclusion of a hyperparameter `lambda`, which controls the learning process (the other parameters, like the predictor variables, are learned in the training phase).

* We'll use the `cv.glmnet()` function to perform a k-fold cross-validation to find an optimal value to use for the hyperparameter `lambda`.  The directions in the tutorial create a matrix of different values to try for the lambda parameter, or we can just let the `cv.glmnet()` function use its own to try.

```{r ridge-regression-housing-data}
# Define the vectors as input for the model
y <- house_sales$AdjSalePrice
x <- house_sales %>% 
  dplyr::select(SqFtTotLiving, Bathrooms, Bedrooms, BldgGrade, 
         PropertyType, SqFtFinBasement, YrBuilt) %>% 
  data.matrix()

# Use cross-validation to find value for lambda
cv_fit <- cv.glmnet(x, y, alpha = 0)

# Plot the mean squared error as a function of the log(lambda)
data.frame(log_lambda = log(cv_fit$lambda), 
           error = cv_fit$cvm, 
           std_error = cv_fit$cvsd) %>% 
  ggplot(aes(log_lambda, error)) +
  geom_errorbar(aes(ymin = error - std_error, 
                    ymax = error + std_error), 
                width = 0.1, 
                color = "grey") +
  geom_point(color = "red") +
  my_theme +
  labs(x = "Log(lambda)", 
       y = "Mean-squared error", 
       title = "Cross-validation results of the mean-squared error as a funcion of lambda")

```

* We can then take the results of this cross-validation procedure and find the value of lambda which minimizes the mean-squared error using `cv_fit$lambda.min` and use that in the ridge regression model.  The error in the cross-validation procedure involves partitioning data into training and validation sets, then evaluating how well the model generalized using the coefficients learned in the training phase.

```{r ridge-regression}
# Use the value of lambda.min in the ridge regression model
fit <- glmnet(x, y, alpha = 0, lambda = cv_fit$lambda.min)
summary(fit)
```
* From here, we can compare the differences in coefficients from the OLS approach with the stepwise multiple linear regression and the ridge regression.

```{r compare-coefficients}
# Compare the coefficients from the OLS and ridge regressions
ridge_df <- as.matrix(fit$beta) %>% 
  data.frame() %>% 
  rownames_to_column("Feature") %>% 
  rename("Coefficient" = "s0") %>% 
  mutate(Model = "Ridge", 
         Coefficient = round(Coefficient, 2))

ols_df <- as.matrix(stepwise_model$coefficients) %>% 
  data.frame() %>% 
  rownames_to_column("Feature") %>% 
  rename("Coefficient" = ".") %>% 
  mutate(Model = "OLS", 
         Coefficient = round(Coefficient, 2)) 

# Combine data frames
rbind(ridge_df, ols_df) %>% 
  pivot_wider(names_from = "Model", 
              values_from = "Coefficient") %>% 
  kbl(caption = "Difference in model coefficients using ridge and ordinary least squares regression") %>% 
  kable_styling(full_width = FALSE, 
                bootstrap_options = "striped") %>% 
  kable_classic_2()
```

* Overall, we can see that in general, the coefficients from the ridge regression model are smaller than the coefficients from the OLS model, and the ridge regression model didn't split `PropertyType` into single family or townhouse.  

* Weighted regression may be used in situations where measurements were taken with different precision.  For example, in the housing data, older home prices are less reliable than newer sales, so we will add a `Weight` field to compute the number of years passed since 2005 to include in our model.  

```{r weighted-regression}
# Create a weight field and calculate time since 2005 for the sale of the home
weighted_lm <- house_sales %>% 
  mutate(DocumentDateYear = year(DocumentDate), 
         Weight = DocumentDateYear - 2005) %>% 
  lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +
       Bedrooms + BldgGrade, data = ., weight = Weight)

# Print the summary information for the regular lm and the weighted lm
cbind(house_lm = round(house_lm$coefficients, 2), 
      weighted_lm = round(weighted_lm$coefficients, 2)) %>% 
  data.frame() %>% 
  rownames_to_column("Feature") %>% 
  rename_all(., str_to_sentence) %>% 
  kbl(caption = "Coefficients for features from normal and weighted linear regression") %>% 
  kable_styling(full_width = FALSE, 
                bootstrap_options = "striped") %>% 
  kable_classic_2()
```


## Prediction Using Regression

* The major motivation behind regression in data science is prediction, but it's important to keep in mind that predictions can only be made within the range of values in the predictor variables that the regression model was trained on.

* Bootstrapping can be used to generate confidence intervals around coefficient estimates by randomly drawing records from the pool (e.g., 100 times to generate a large sample), fitting the bootstrap sample data to a regression model and recording the output, then repeat those two steps a number of times (e.g., 1,000) to generate a distribution of possible coefficient estimates for each parameter.  The 95% confidence interval then is constructed by finding the appropriate percentiles for the desired level of confidence (e.g., 95% confidence interval).  We can use the `tidymodels::bootstraps()` function to generate bootstrap samples of our original data, fit linear regression models (or other models) to the samples, and evaluate the distribution of the coefficient estimates. 

```{r coefficient-confidence-intervals}
# Generate 1,000 bootstrap sample sets
boots <- bootstraps(house_sales_subset, times = 1000)
class(boots)

# The result is an rset object, which consists of an analysis data set (the bootstrapped sample) and the assessment data set (the out of bag samples, or the ones not chosen in the sampling process)

# Now, we'll create a function that will fit a linear model to the bootstrap samples, using the analysis() function, which takes the boots object and only grabs the bootstrap samples (in-the-bag samples)
fit_lm_to_boots <- function(split) {
  lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + Bedrooms + BldgGrade, 
     data = analysis(split))
}

# Now map this function to each bootstrap sample, plus the tidy() function to get the coefficients from the models
boots_models <- boots %>% 
  mutate(model = map(splits, fit_lm_to_boots), 
         coef_info = map(model, tidy))

# Unnest the coefficient info 
boots_coefs <- boots_models %>% 
  unnest(coef_info)

# Calculate 95% confidence intervals using the percentile method
percentile_intervals <- int_pctl(boots_models, coef_info)

# Visualize the distribution of the coefficients with 95% confidence intervals
boots_coefs %>% 
  dplyr::select(-c("splits", "model")) %>% 
  ggplot(aes(estimate)) +
  geom_histogram(bins = 30) +
  geom_vline(aes(xintercept = .lower), data = percentile_intervals, color = "red", linetype = "dashed") +
  geom_vline(aes(xintercept = .upper), data = percentile_intervals, color = "red", linetype = "dashed") +
  facet_wrap(~ term, scales = "free") +
  my_theme +
  theme(
    strip.background = element_rect(color = "black", fill = "lightskyblue"), 
    strip.text = element_text(face = "bold")
  ) +
  labs(x = "Estimate", 
       y = "Count", 
       title = "Distribution of linear model coefficient estimates from 1,000 bootstrapped samples")
```

* A key consideration when interpreting prediction versus confidence intervals is that prediction intervals model uncertainty around a single point; confidence intervals model uncertainty around a mean calculated from a range of values.  

## Factor Variables in Regression

* Linear regression models require numeric inputs, but some features may be categorical, and can be converted to a "dummy variable" or a numeric variable.

* In certain machine learning applications, we can use the `model.matrix()` function to create dummy variables for each of the features.  For example, if there are three categorical variables for one feature, the function would expand that vector into a three-column matrix, with the value for a "yes" or "no" for that category of feature represented as a 1 or 0 for each record.

* In regression, the first level of the categorical variable is used as a reference, and we interpret the coefficients for the other levels relative to that reference.  For example in the housing data, there three levels to the `PropertyType` field: `Multiplex`, `Single Family`, and `Townhouse`.  

* The regression coefficients from the stepwise model are for `PropertyType Single Family` and `PropertyType Townhouse`, indicating that we need to interpret these coefficients in reference to `PropertyType Mutliplex` (see below).

```{r stepwise-coefficients}
# Print the stepwise coefficients to compare the estimates to the PropertyType Multiplex level
house_sales_lm <- house_sales %>% 
  lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +
       Bedrooms + BldgGrade + PropertyType, data = .) 

# Get coefficients and print
data.frame(coefficients = house_sales_lm$coefficients) %>% 
  rownames_to_column("feature") %>%  
  kable_layout(caption = "Coefficient estimates from stepwise linear regression model")

```

* Based on these coefficient estimates, relative to multiplex homes, single family homes were worth approximately 85,000 USD less (holding all other factors constant), and townhouses were worth approximately 150,000 USD less.   

## Interpreting the Regression Equation
 
* It's important to consider that some variables may be correlated, like the total size of the home, the numnber of bedrooms, and the number of bathrooms- this would make sense, because the number of bedrooms and bathrooms will generally increase with the size of the home.

* Multicollinearity is an extreme case of correlation among predictor variables, which can happen when multiple predictors are nearly perfectly correlated with one another and will make it difficult to obtain a salient solution for the regression equation.

* Multicollinearity is a major problem because it means that any change in the input data can dramatically change the coefficient estimates.  This would also mean that it would be hard for you to interpret how the response would change with a single change in the predictor variable.  To investigate multicollinearity among predictors, we could create a correlation plot (see below).

```{r correlation-plot-multicollinearity}
# Create a correlation plot to detect multicollinearity
cor(house_sales_subset) %>% 
  ggcorrplot(legend.title = "Correlation", 
             outline.color = "white", 
             lab = TRUE, 
             type = "lower", 
             hc.order = TRUE)

```

* The correlation plot shows that some of the variables are correlated (~0.77), like the square feet of living space and the building grade, along with the square feet of living space and the number of bathrooms and bedrooms.

* Confounding variables can also be a problem, which is when there are variables that are important in explaining the variability in the data but are not included in the model (e.g., location in the housing data set).

* Main effects are the effects of the predictor variables on the response independent of any of the other predictor variables; interactions are the effects of different predictors on one another to influence the response (e.g., in the housing data, there is almost definitely an interaction between the location and size of the house that will influence the overall price).

* In the housing data, there was a clear relationship between the square footage of the home and the location- for example, in the most expensive areas, the interaction between the location and square footage increased the predicted sales price to almost a factor of 3.  

* Tree models may be useful options to consider when dealing with many potential interaction terms.  
 
## Regression Diagnostics
 
## Polynomial and Spline Regression
 
 
