---
title: "Chapter 4: Regression and Prediction"
output:
  html_notebook:
    toc: true
    toc_float: true
    theme: cerulean
    highlight: tango
---

```{r setup, warning = FALSE, message = FALSE}
# Load libraries
library(tidyverse)
library(forcats)
library(ggtext)
library(ggrepel)
library(scales)
library(janitor)
library(kableExtra)
library(broom)
library(patchwork)
library(lubridate)
library(MASS)
library(glmnet)

# Set ggplot2 theme
my_theme <- theme(
  panel.background = element_rect(color = "black", fill = "white"), 
  panel.grid = element_blank(),
  plot.title = element_markdown(),
  plot.subtitle = element_markdown(),
  plot.caption = element_markdown(face = "italic")
)

```


# Introduction

This chapter focuses on prediction using regression, or identifying the quantitative relationship between a number of predictor variables and a response.  This is a form of supervised learning, in which we are training a statistical model with known outcomes in order to generalize to unknown, or unseen data, within the bounds of the predictor variables in the training data set.  We will also explore the subfield of anomaly detection using the relationship between the predictors and responses to identify responses that are extreme or unusual (e.g., outside the range of values that would be expected 95% of the time). 

## Simple Linear Regression

Notes:

* Regression is similar to correlation in that we're quantifying the relationship between one or more predictor variables and a response variable; the difference is that correlation measures the strength of the relationship (e.g., 0.99 correlation), while regression gives us the nature of the relationship (e.g., for every one unit increase in X, Y increases by 2.5 units).

* Importantly, the y-intercept is the value of the resonse variable when X = 0; in other words, this is the "default" or average baseline response (e.g., when X = 0, the y-intercept may be $150,000 for home prices in Seattle, meaning that without considering square footage or anything else, that's the "base price" to build off of). 

* The plot below shows the relationship between peak expiratory flow rate and exposure to cotton dust from the `LungDisease.csv` dataset (more information about the hazards of lung disease and obstructive dust from handling and processing cotton can be found on the [CDC website](https://www.labor.nc.gov/cotton-dust#hazard-overview)).

```{r lung-disease-eda}
# Import the data
lung_disease <- read_csv("../data/LungDisease.csv")

# Generate the scatterplot
lung_disease %>% 
  ggplot(aes(Exposure, PEFR)) +
  geom_point() +
  my_theme +
  labs(title = "Relationship between exposure to cotton dust and peak expiratory flow rate (PEFR)")
```

* The relationship is difficult to discern just looking at the scatterplot, so we can use the `lm()` function in R to construct a linear model and quantify the relationship between PEFR and exposure.

```{r regression-model-1}
# Create a linear regression model for exposure (predictor) and PEFR (response)
mod1 <- lm(PEFR ~ Exposure, data = lung_disease)

# Explore the summary information from the model output
summary(mod1) %>% 
  tidy() %>% 
  kbl(caption = "Coefficient estimates from a simple linear regression model between exposure (predictor) and PEFR (response)") %>% 
  kable_styling(full_width = FALSE,
                bootstrap_options = "striped") %>% 
  kable_classic_2()
```
* From this output, we can see that both the `Intercept` and `Exposure` terms are statistically significant and are informative in the model.

* When interpreting the coefficients, without taking into account any cotton dust exposure, the mean PEFR is approximately 424.

* When we take into account exposure, for every one unit increase in exposure to cotton dust, there is a 4 unit decrease in PEFR.  

* The residuals are calculated by subtracting each predicted value from the original value and serves as an indication of how well the model explains the variability in the response data.  

* We can add both the predictions and the residuals as fields in the original data frame using the `modelr` package (see below).

```{r add-predictions-residuals}
# Add predicted values and residuals to the original data frame
lung_disease_mod <- lung_disease %>% 
  add_predictions(model = mod1) %>% 
  add_residuals(model = mod1)

# Plot the predicted values and the original data
lung_pred_plot <- lung_disease_mod %>% 
  pivot_longer(cols = c("PEFR", "pred", "resid"), 
               names_to = "type", 
               values_to = "value") %>% 
  filter(type %in% c("PEFR", "pred")) %>% 
  ggplot(aes(Exposure, value)) +
  geom_point(aes(color = type), alpha = 0.75) +
  scale_color_manual(values = c("darkgrey", "red")) +
  my_theme +
  theme(legend.position = "none") +
  labs(y = "Value", 
       title = "Relationship between exposure to cotton dust and PEFR", 
       subtitle = "<strong style='color:darkgrey';>Original data</strong> are shown in grey; 
       <strong style='color:red';>predicted values</strong> are shown in red")

# Plot the residuals
lung_residuals_plot <- lung_disease_mod %>% 
  pivot_longer(cols = c("PEFR", "pred", "resid"), 
               names_to = "type", 
               values_to = "value") %>% 
  filter(type %in% c("resid")) %>% 
  ggplot(aes(Exposure, value)) +
  geom_point() +
  geom_hline(yintercept = 0, 
             linetype = "dashed", 
             color = "red") +
  my_theme +
  labs(y = "Value", 
       title = "Residuals from the linear model between exposure and PEFR")

# Put plots together
lung_pred_plot
```

* From the plot above, we can see the predicted values from the linear model shown in red (i.e., the mean response for each value of the predictor variable) and the original data shown in grey.

* There appears to be considerable variability in the mean response; we can dig into this more deeply looking at a plot of the residuals, or the difference between the predicted value and the original data.

```{r residuals-plot}
# Plot the residuals
lung_residuals_plot

```

* We can see that the dashed line at Y = 0 and ideally, if the linear model described the variation in the original data, most of the points would cluster around that dashed line.  We can see that especially at higher levels of exposure, there is considerable variability.

* The regression coefficients are chosen by minimizing the residual sum of squares (RSS), or coefficients which minimize the sum of the squared distances between the original data and the predicted value.

* Using linear regression to find coefficient estimates which minimize the RSS is called least squares regression or ordinary least squares regression.

## Multiple Linear Regression

Notes:

* The overall concepts are the same for multiple linear regression as they are for simple linear regression, except now we're dealing with more than one predictor.

* For example, we can look at the `house_sales.csv` data set, which contains data on the adjusted sale price for homes in King County, Washington, along with a number of different predictors including square feet of living space, square feet of the lot, the number of bedrooms and bathrooms in different fields, and the building grade.  Multiple linear regression can help us to see the relative contributions of each predictor in explaining the variability in the response.  Note, the data set needed to be reformatted prior to importing as the fields were shifted.

```{r king-county-housing-model}
# Import the king county dataset
house_sales <- read_csv("../data/house_sales.csv")

# Reformat the date field
house_sales <- house_sales %>% 
  mutate(DocumentDate = as_date(DocumentDate, format = "%m/%d/%y"))

# Isolate fields of interest
house_sales_subset <- house_sales %>% 
  select(AdjSalePrice, SqFtTotLiving, SqFtLot, Bathrooms, Bedrooms, BldgGrade)

# Build linear model and drop records with missing data
house_lm <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + Bedrooms + BldgGrade, 
               data = house_sales_subset, 
               na.action = na.omit)

# Print model results
summary(house_lm) %>% 
  tidy() %>% 
  mutate(across(where(is.numeric), round)) %>% 
  kbl(caption = "Model results for housing data in King County, Washington (rounded to the nearest integer)") %>% 
  kable_styling(full_width = FALSE, 
                bootstrap_options = "striped") %>% 
  kable_classic_2()
```

* After constructing the multiple linear regression, we can see that all of the terms are statistically significant except the `SqFtLot` term, which defines the square feet of the lot (the p-value is greater than 0.05).  

* Not surprisingly, it looks like the terms with the greatest influence, or the greatest change in the response (adjusted sales price) with each change in their value, are the square feet of total living space and building grade.

* In the context of the data set, for every one square foot increase in total living space, the mean adjusted sales price rises $229.  

* We can get other information about the model using `glance()`.

```{r glance-house-model}
# Use glance to gather additional summary information about the model
glance(house_lm) %>% 
  kbl(caption = "Summary information from the model evaluating adjusted home sale price in King County, Washington") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()

```

* Looking at the R-squared and adjusted R-squared, the current predictors only explain a little over 50% of the variability in the response.  

* Cross-validation is used to set aside a training set (usually most of the data) to train the statistical model, and then apply it to a new, unseen set (the validation set).

* The adjusted R-squared adds a penalty term to the model in an effort to prevent overfitting; the AIC value also penalizes adding an additional terms.  Since the AIC method uses the logarithm of the residual sums of squares, a _k_ increase in the number of additional variables equates to a $2K$ penalty for the model.  

* Stepwise regression can be performed, either by adding in variables one at a time (forward selection), or starting with all variables and removing them one at a time (backward elimination) to evaluate how the different combinations influence model fit using the metrics defined above.  We can also use the `stepAIC()` in the `MASS` package to evaluate models based on the AIC value.

```{r stepwise-regression-AIC}
# Use the stepwise AIC method to identify the "best" model with a few additional fields
house_full <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + 
                   Bedrooms + BldgGrade + PropertyType + NbrLivingUnits +
                   SqFtFinBasement + YrBuilt + YrRenovated + NewConstruction, 
                 data = house_sales,
                 na.action = na.omit)

# Perform the stepwise regression 
stepwise_model <- stepAIC(house_full, 
                          direction = "both", 
                          trace = FALSE)

# Get the summary information from the ANOVA
stepwise_model$anova
```

* From the stepwise AIC selection tool, we can see the initial model with all of the variables, then the final model with the lowest AIC including fields for square feet of total living space, number of bedrooms and bathrooms, building grade, property type, square feet in the basement, and the year built (the coefficients for the model terms are presented below).

```{r stepwise-model-terms}
# Print the coefficients for the final model
stepwise_model %>% 
  tidy() %>% 
  kbl(caption = "Coefficients for the final model after performing stepwise regression using AIC") %>% 
  kable_styling(full_width = FALSE, 
                bootstrap_options = "striped") %>% 
  kable_classic_2()
```

* Other model selection methods like ridge regression or LASSO regression penalize the inclusion of too many parameters, effectively shrinking the coefficients towards zero such that only the parameters that explain a relatively large proportion of the variability (i.e., the most important terms) are included in the model.

* [This blogpost](https://drsimonj.svbtle.com/ridge-regression-with-glmnet) has a great tutorial for using ridge regression over OLS regression, which I've implemented on the housing data below using the same parameters as the stepwise AIC input.  One important component of the ridge regression model is the inclusion of a hyperparameter `lambda`, which controls the learning process (the other parameters, like the predictor variables, are learned in the training phase).

* We'll use the `cv.glmnet()` function to perform a k-fold cross-validation to find an optimal value to use for the hyperparameter `lambda`.  The directions in the tutorial create a matrix of different values to try for the lambda parameter, or we can just let the `cv.glmnet()` function use its own to try.

```{r ridge-regression-housing-data}
# Define the vectors as input for the model
y <- house_sales$AdjSalePrice
x <- house_sales %>% 
  dplyr::select(SqFtTotLiving, Bathrooms, Bedrooms, BldgGrade, 
         PropertyType, SqFtFinBasement, YrBuilt) %>% 
  data.matrix()

# Use cross-validation to find value for lambda
cv_fit <- cv.glmnet(x, y, alpha = 0)

# Plot the mean squared error as a function of the log(lambda)
data.frame(log_lambda = log(cv_fit$lambda), 
           error = cv_fit$cvm, 
           std_error = cv_fit$cvsd) %>% 
  ggplot(aes(log_lambda, error)) +
  geom_errorbar(aes(ymin = error - std_error, 
                    ymax = error + std_error), 
                width = 0.1, 
                color = "grey") +
  geom_point(color = "red") +
  my_theme +
  labs(x = "Log(lambda)", 
       y = "Mean-squared error", 
       title = "Cross-validation results of the mean-squared error as a funcion of lambda")

```

* We can then take the results of this cross-validation procedure and find the value of lambda which minimizes the mean-squared error using `cv_fit$lambda.min` and use that in the ridge regression model.  The error in the cross-validation procedure involves partitioning data into training and validation sets, then evaluating how well the model generalized using the coefficients learned in the training phase.

```{r ridge-regression}
# Use the value of lambda.min in the ridge regression model
fit <- glmnet(x, y, alpha = 0, lambda = cv_fit$lambda.min)
coef(fit)
```


## Prediction Using Regression
 
## Factor Variables in Regression
 
## Interpreting the Regression Equation
 
## Regression Diagnostics
 
## Polynomial and Spline Regression
 
 
